{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14064103,"sourceType":"datasetVersion","datasetId":8951868}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport random\n\n# -------------------------------------------------------------------\n# 1) Data Sources (Improved Questions + Better Answer Structures)\n# -------------------------------------------------------------------\n\ncategories = {\n    \"idea_validation\": [\n        \"How can I validate my startup idea for a {}?\",\n        \"What are the fastest ways to test if my {} idea is worth building?\",\n        \"How do founders usually validate early concepts in the {} space?\",\n        \"Is my {} concept viable, and how do I find out cheaply?\"\n    ],\n\n    \"business_model\": [\n        \"Which business model fits a {} best?\",\n        \"Should I choose subscription or freemium pricing for my {}?\",\n        \"What are profitable monetization strategies for a {} startup?\",\n        \"How do similar {} startups structure their revenue streams?\"\n    ],\n\n    \"pricing_strategy\": [\n        \"How do I price my {} product effectively?\",\n        \"What pricing model works best for {} founders?\",\n        \"How to determine recurring subscription fees for a {}?\",\n        \"Should my {} pricing be usage-based or tier-based?\"\n    ],\n\n    \"customer_acquisition\": [\n        \"How do I get my first 100 paying users for a {}?\",\n        \"What are the highest-ROI marketing channels for a {} startup?\",\n        \"How can I grow my {} user base quickly with a small budget?\",\n        \"Which acquisition strategies work best for a {} MVP?\"\n    ],\n\n    \"mvp_features\": [\n        \"Which features should be included in the first MVP for a {}?\",\n        \"What are the must-have features when building a {} prototype?\",\n        \"How do I prioritize features for a {} MVP launch?\",\n        \"What is the minimal feature set needed to test a {}?\"\n    ],\n\n    \"marketing_channels\": [\n        \"What are effective marketing channels for promoting a {}?\",\n        \"How can a {} startup use social media for growth?\",\n        \"What channels produce consistent traction for {} founders?\",\n        \"Which channels give best ROI for early-stage {} products?\"\n    ],\n\n    \"tech_stack\": [\n        \"Which tech stack should I use to build a {} MVP?\",\n        \"What is the best frontend/backend stack for a {}?\",\n        \"Should I choose React or Vue when building a {} product?\",\n        \"How do I choose the right database and backend for a {}?\"\n    ]\n}\n\n# -------------------------------------------------------------------\n# 2) Richer Answer Templates â€” More Natural + Multi-Sentence\n# -------------------------------------------------------------------\n\nanswer_templates = {\n    \"idea_validation\": [\n        \"Start by speaking with potential users, running short interviews, and validating the core problem. Then create a simple prototype or landing page to collect signups and measure real interest. This helps you avoid building something nobody needs.\",\n        \"Use low-cost methods like surveys, no-code prototypes, and competitor analysis to understand whether people actually face the problem. Real feedback and early user conversations are the strongest validation signals.\"\n    ],\n\n    \"business_model\": [\n        \"A subscription model works well if customers receive continuous value, while a freemium approach helps acquire a wide audience fast. Study competitor pricing and test small variations to see what drives conversions.\",\n        \"Choose a monetization strategy based on customer type and market patterns. Some products grow well with usage-based pricing, while others rely on flat monthly fees for predictability.\"\n    ],\n\n    \"pricing_strategy\": [\n        \"Benchmark competitors, estimate your cost structure, and experiment with pricing tiers. Founders often start with affordable entry-level tiers and then adjust pricing as the product matures.\",\n        \"Use customer interviews, small pricing tests, and clear value-based positioning. Premium tiers should highlight strong differentiators, while lower tiers attract early adopters.\"\n    ],\n\n    \"customer_acquisition\": [\n        \"Focus on channels where your audience already spends time. Use targeted content, communities, small paid ads, and referral incentives. Early traction often comes from consistent engagement, not large budgets.\",\n        \"Start with organic channels like social media, partnerships, and authority-building content. Once you understand what converts, scale using paid acquisition or influencer collaborations.\"\n    ],\n\n    \"mvp_features\": [\n        \"Include only the features required to solve the main problem your user faces. Avoid perfection. Early MVPs succeed when they are fast, simple, and focused on the core value.\",\n        \"Prioritize features that demonstrate the productâ€™s core utility. Anything that doesnâ€™t validate user need belongs in later iterations.\"\n    ],\n\n    \"marketing_channels\": [\n        \"Content marketing, social media engagement, and community-building are powerful channels for early-stage founders. Build trust before pushing for conversions.\",\n        \"Email marketing, SEO, and collaborations with micro-influencers can generate consistent results. Choose channels based on where your target persona spends time.\"\n    ],\n\n    \"tech_stack\": [\n        \"Pick a stack that allows rapid iteration. React or Vue for frontend, Node.js or Django for backend, and PostgreSQL for stable data storage are common startup choices.\",\n        \"Choose tools that match your team's experience. Fast development cycles matter more than the specific framework, especially in MVP stages.\"\n    ]\n}\n\n# -------------------------------------------------------------------\n# 3) Domains + Tags\n# -------------------------------------------------------------------\n\ndomains = [\n    \"SaaS platform\", \"mobile app\", \"AI tool\", \"marketplace\",\n    \"web app\", \"fintech solution\", \"e-commerce startup\",\n    \"productivity tool\", \"health tech app\", \"education platform\"\n]\n\ntag_options = [\n    \"startup\", \"SaaS\", \"AI\", \"product\", \"pricing\",\n    \"marketing\", \"tech stack\", \"MVP\", \"funding\",\n    \"customer acquisition\", \"growth\"\n]\n\n# -------------------------------------------------------------------\n# 4) Synthetic Dataset Generator (Enhanced)\n# -------------------------------------------------------------------\n\ndataset = []\nTARGET_SIZE = 10000   # You may increase to 5k / 10k / 50k easily\n\nwhile len(dataset) < TARGET_SIZE:\n\n    category = random.choice(list(categories.keys()))\n    domain = random.choice(domains)\n\n    question_template = random.choice(categories[category])\n    question = question_template.format(domain)\n\n    answer_base = random.choice(answer_templates[category])\n\n    # Add natural variation\n    variation = random.choice([\n        \"\",\n        \" Consider testing variations to see what users respond to.\",\n        \" Early feedback will guide your next development steps.\",\n        \" Founders often underestimate how important talking to users is.\",\n        \" You can always refine the strategy as your product evolves.\"\n    ])\n\n    answer = f\"For your {domain}, {answer_base}{variation}\"\n\n    tags = random.sample(tag_options, k=random.randint(2,4))\n\n    dataset.append({\n        \"question\": question,\n        \"answer\": answer,\n        \"category\": category,\n        \"tags\": tags\n    })\n\n# -------------------------------------------------------------------\n# 5) Save As JSON\n# -------------------------------------------------------------------\n\nwith open(\"synthetic_startup_qa_1000.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(dataset, f, indent=4, ensure_ascii=False)\n\nprint(f\"âœ… Generated {TARGET_SIZE} enhanced startup Q&A samples â†’ synthetic_startup_qa_1000.json\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:01:25.830878Z","iopub.execute_input":"2025-12-12T11:01:25.831165Z","iopub.status.idle":"2025-12-12T11:01:26.035689Z","shell.execute_reply.started":"2025-12-12T11:01:25.831142Z","shell.execute_reply":"2025-12-12T11:01:26.034850Z"}},"outputs":[{"name":"stdout","text":"âœ… Generated 10000 enhanced startup Q&A samples â†’ synthetic_startup_qa_1000.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport os\nimport kagglehub\n\n# -----------------------------------------------------------\n# 1. Load EXISTING synthetic dataset\n# -----------------------------------------------------------\nsynthetic_file = \"synthetic_startup_qa_1000.json\"\n\nif os.path.exists(synthetic_file):\n    with open(synthetic_file, \"r\", encoding=\"utf-8\") as f:\n        synthetic_data = json.load(f)\nelse:\n    synthetic_data = []\n    print(\"âš ï¸ synthetic_startup_qa_1000.json not found. Starting with empty list.\")\n\nprint(\"Loaded synthetic dataset:\", len(synthetic_data))\n\n# -----------------------------------------------------------\n# 2. Download Kaggle dataset\n# -----------------------------------------------------------\nprint(\"Downloading Kaggle dataset...\")\npath = kagglehub.dataset_download(\"abhayayare/multi-turn-chatbot-conversation-dataset\")\nprint(\"Dataset downloaded to:\", path)\n\n# Expecting a CSV inside\ncsv_path = None\nfor file in os.listdir(path):\n    if file.endswith(\".csv\"):\n        csv_path = os.path.join(path, file)\n        break\n\nif csv_path is None:\n    raise ValueError(\"âŒ No CSV file found in downloaded Kaggle dataset.\")\n\n# Load the Kaggle file\ndf = pd.read_csv(csv_path)\n\nprint(\"Loaded Kaggle rows:\", len(df))\n\n# -----------------------------------------------------------\n# 3. Convert multi-turn chats into userâ†’bot Q&A pairs\n# -----------------------------------------------------------\n\n# We assume dataset columns: conversation_id, turn, role, intent, message\n\nmerged_pairs = []\n\ngrouped = df.groupby(\"conversation_id\")\n\nfor conv_id, conv in grouped:\n    conv = conv.sort_values(\"turn\")\n\n    last_user_msg = None\n\n    for _, row in conv.iterrows():\n\n        if row[\"role\"] == \"user\":\n            last_user_msg = row[\"message\"]\n\n        elif row[\"role\"] == \"bot\" and last_user_msg:\n            # Build a dataset entry\n            merged_pairs.append({\n                \"question\": str(last_user_msg).strip(),\n                \"answer\": str(row[\"message\"]).strip(),\n                \"category\": \"general_chat\",\n                \"tags\": [\"chat\", \"conversation\"]\n            })\n            last_user_msg = None\n\nprint(\"Extracted userâ†’bot pairs:\", len(merged_pairs))\n\n# -----------------------------------------------------------\n# 4. Merge with existing synthetic dataset\n# -----------------------------------------------------------\n\nfinal_dataset = synthetic_data + merged_pairs\n\nprint(\"Final dataset size:\", len(final_dataset))\n\n# -----------------------------------------------------------\n# 5. Save into the SAME FILE NAME (your requirement)\n# -----------------------------------------------------------\n\nwith open(\"synthetic_startup_qa_1000.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(final_dataset, f, indent=4, ensure_ascii=False)\n\nprint(\"âœ… Saved merged dataset â†’ synthetic_startup_qa_1000.json\")\nprint(\"ðŸŽ‰ Dataset ready for training!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:01:26.037187Z","iopub.execute_input":"2025-12-12T11:01:26.037469Z","iopub.status.idle":"2025-12-12T11:17:41.086039Z","shell.execute_reply.started":"2025-12-12T11:01:26.037438Z","shell.execute_reply":"2025-12-12T11:17:41.085232Z"}},"outputs":[{"name":"stdout","text":"Loaded synthetic dataset: 10000\nDownloading Kaggle dataset...\nDataset downloaded to: /kaggle/input/multi-turn-chatbot-conversation-dataset\nLoaded Kaggle rows: 12996120\nExtracted userâ†’bot pairs: 6498060\nFinal dataset size: 6508060\nâœ… Saved merged dataset â†’ synthetic_startup_qa_1000.json\nðŸŽ‰ Dataset ready for training!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport json\nimport math\nimport random\n\n# ---- Add this ----\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:17:41.086749Z","iopub.execute_input":"2025-12-12T11:17:41.087006Z","iopub.status.idle":"2025-12-12T11:17:45.464652Z","shell.execute_reply.started":"2025-12-12T11:17:41.086984Z","shell.execute_reply":"2025-12-12T11:17:45.463757Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import math\nimport random\nimport json\nfrom pathlib import Path\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# -------------------------\n# Config / Hyperparameters\n# -------------------------\nCFG = {\n    \"seq_len\": 256,\n    \"batch_size\": 64,\n    \"epochs\": 9,\n    \"lr\": 2e-3,\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    # Model size (scale down for limited GPU memory)\n    \"d_model\": 384,\n    \"n_layers\": 8,\n    \"n_heads\": 6,\n    \"d_ff\": 1536,\n    \"dropout\": 0.1,\n    \"print_every_batches\": 100,\n    \"epoch_sample\": 200_000\n}\n# -------------------------\n# 1) Simple word-level tokenizer (from scratch)\n# -------------------------\nclass WordTokenizer:\n    def __init__(self, texts: List[str], min_freq: int = 1):\n        self.specials = [\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\", \"<SEP>\"]\n        freq = {}\n        for t in texts:\n            for w in self._pre_tokenize(t):\n                freq[w] = freq.get(w, 0) + 1\n        vocab = [w for w,c in freq.items() if c >= min_freq]\n        vocab = sorted(vocab)\n        self.itos = self.specials + vocab\n        self.stoi = {w:i for i,w in enumerate(self.itos)}\n        self.pad_id = self.stoi[\"<PAD>\"]\n        self.unk_id = self.stoi[\"<UNK>\"]\n        self.bos_id = self.stoi[\"<BOS>\"]\n        self.eos_id = self.stoi[\"<EOS>\"]\n        self.sep_id = self.stoi[\"<SEP>\"]\n        self.vocab_size = len(self.itos)\n\n    def _pre_tokenize(self, text: str):\n        # simple splitter: keep punctuation as separate tokens\n        tokens = []\n        cur = \"\"\n        for ch in text:\n            if ch.isalnum():\n                cur += ch.lower()\n            else:\n                if cur:\n                    tokens.append(cur)\n                    cur = \"\"\n                if not ch.isspace():\n                    tokens.append(ch)\n        if cur:\n            tokens.append(cur)\n        return tokens\n\n    def encode(self, text: str, add_specials=True) -> List[int]:\n        toks = self._pre_tokenize(text)\n        ids = [self.stoi.get(t, self.unk_id) for t in toks]\n        if add_specials:\n            ids = [self.bos_id] + ids + [self.eos_id]\n        return ids\n\n    def decode(self, ids: List[int]) -> str:\n        words = []\n        for i in ids:\n            if i < 0 or i >= len(self.itos):\n                words.append(\"<UNK>\")\n            else:\n                w = self.itos[i]\n                if w in self.specials:\n                    continue\n                words.append(w)\n        # join tokens: add spaces around alnum tokens, keep punctuation\n        out = \"\"\n        for tok in words:\n            if len(tok) == 1 and not tok.isalnum():\n                out = out + tok\n            else:\n                if out:\n                    out += \" \"\n                out += tok\n        return out\n\n# -------------------------\n# 2) Dataset: keep samples separate and pad\n# -------------------------\nclass QADataset(Dataset):\n    def __init__(self, qa_pairs: List[dict], tokenizer: WordTokenizer, seq_len: int):\n        self.samples = []\n        self.tokenizer = tokenizer\n        self.seq_len = seq_len\n        for item in qa_pairs:\n            q = item.get(\"question\", \"\")\n            a = item.get(\"answer\", \"\")\n            # Format: <BOS> question <SEP> answer <EOS>\n            ids = [tokenizer.bos_id] + tokenizer.encode(q, add_specials=False) + [tokenizer.sep_id] + tokenizer.encode(a, add_specials=False) + [tokenizer.eos_id]\n            # truncate or keep full (we will pad later)\n            if len(ids) > seq_len:\n                ids = ids[:seq_len]\n                if ids[-1] != tokenizer.eos_id:\n                    ids[-1] = tokenizer.eos_id\n            self.samples.append(ids)\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        ids = self.samples[idx]\n        L = len(ids)\n\n        # Always pad BEFORE shifting\n        pad_len = self.seq_len - L\n        if pad_len > 0:\n            ids = ids + [self.tokenizer.pad_id] * pad_len\n\n        # Shift AFTER padding (correct)\n        input_ids = ids[:-1]            # 0..126\n        target_ids = ids[1:]            # 1..127\n\n        attention_mask = [1 if t != self.tokenizer.pad_id else 0 for t in input_ids]\n\n        return {\n            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n            \"labels\": torch.tensor(target_ids, dtype=torch.long),\n            \"attn_mask\": torch.tensor(attention_mask, dtype=torch.long),\n        }\n\n\n\n# -------------------------\n# 3) Transformer building blocks (fixed masks + padding-aware attention)\n# -------------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        # x: (B, T, D)\n        return x + self.pe[:, : x.size(1), :]\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, n_heads):\n        super().__init__()\n        assert d_model % n_heads == 0\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n        self.q_linear = nn.Linear(d_model, d_model)\n        self.k_linear = nn.Linear(d_model, d_model)\n        self.v_linear = nn.Linear(d_model, d_model)\n        self.fc = nn.Linear(d_model, d_model)\n\n    def forward(self, x, attn_mask=None):\n        B, T, D = x.size()\n        Q = self.q_linear(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n        K = self.k_linear(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n        V = self.v_linear(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n\n        # Q,K: (B, heads, T, d_k)\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        # attn_mask expected shape: (B, 1, T, T) where 1=for broadcast over heads\n        if attn_mask is not None:\n            # mask should have 0 for positions to mask\n            scores = scores.masked_fill(attn_mask == 0, float(\"-inf\"))\n        attn = F.softmax(scores, dim=-1)\n        out = torch.matmul(attn, V)  # (B, heads, T, d_k)\n        out = out.transpose(1, 2).contiguous().view(B, T, D)\n        return self.fc(out)\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.attn = MultiHeadAttention(d_model, n_heads)\n        self.ln1 = nn.LayerNorm(d_model)\n        self.ff = nn.Sequential(nn.Linear(d_model, d_ff), nn.ReLU(), nn.Linear(d_ff, d_model))\n        self.ln2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, attn_mask=None):\n        # pre-norm\n        x2 = self.ln1(x)\n        x = x + self.dropout(self.attn(x2, attn_mask=attn_mask))\n        x2 = self.ln2(x)\n        x = x + self.dropout(self.ff(x2))\n        return x\n\n\nclass EpochSampler(torch.utils.data.Sampler):\n    def __init__(self, dataset_size, epoch_sample):\n        self.dataset_size = dataset_size\n        self.epoch_sample = min(epoch_sample, dataset_size)\n\n    def __iter__(self):\n        # randomly choose \"epoch_sample\" unique indices every epoch\n        return iter(random.sample(range(self.dataset_size), self.epoch_sample))\n\n    def __len__(self):\n        return self.epoch_sample\n\n\nclass SimpleGPT(nn.Module):\n    def __init__(self, vocab_size, max_seq_len, d_model, n_heads, n_layers, d_ff, dropout=0.1):\n        super().__init__()\n        self.token_emb = nn.Embedding(vocab_size, d_model)\n        self.pos_emb = PositionalEncoding(d_model, max_len=max_seq_len)\n        self.layers = nn.ModuleList([TransformerBlock(d_model, n_heads, d_ff, dropout=dropout) for _ in range(n_layers)])\n        self.ln_f = nn.LayerNorm(d_model)\n        self.head = nn.Linear(d_model, vocab_size)\n\n    def forward(self, input_ids, attn_mask=None):\n        # input_ids: (B, T)\n        x = self.token_emb(input_ids)  # (B, T, D)\n        x = self.pos_emb(x)\n        for layer in self.layers:\n            x = layer(x, attn_mask)\n        x = self.ln_f(x)\n        logits = self.head(x)\n        return logits\n\n# -------------------------\n# 4) Masks helper: combine causal mask + padding mask\n# -------------------------\n\ndef build_attn_mask(attn_mask_pad: torch.Tensor, device: torch.device):\n    # attn_mask_pad: (B, T) with 1 for tokens, 0 for pad\n    B, T = attn_mask_pad.shape\n    # causal mask (T, T)\n    causal = torch.tril(torch.ones((T, T), device=device)).unsqueeze(0).unsqueeze(1)  # (1,1,T,T)\n    # padding mask: (B, 1, 1, T) -> broadcast to (B,1,T,T)\n    pad_mask = attn_mask_pad.unsqueeze(1).unsqueeze(2)  # (B,1,1,T)\n    pad_mask = pad_mask.expand(B, 1, T, T)  # (B,1,T,T)\n    attn_mask = causal * pad_mask\n    return attn_mask  # values 0/1\n\n# -------------------------\n# 5) Generation helper (temperature + top-k)\n# -------------------------\n\ndef top_k_logits(logits, k):\n    if k == 0 or k >= logits.size(-1):\n        return logits\n\n    # Works for both shape (V) and (1,V)\n    v, ix = torch.topk(logits, k)\n    cutoff = v[..., -1].unsqueeze(-1)\n    return torch.where(logits < cutoff, torch.full_like(logits, -1e10), logits)\n\n\ndef generate(model, tokenizer: WordTokenizer, prompt: str, max_new_tokens=100, temperature=1.0, top_k=40, device=None):\n    model.eval()\n    device = device or next(model.parameters()).device\n\n    ids = tokenizer.encode(prompt, add_specials=True)\n    ids = ids[:CFG['seq_len']]\n    ids = ids + [tokenizer.pad_id] * (CFG['seq_len'] - len(ids))\n    input_ids = torch.tensor([ids], device=device)\n\n    for _ in range(max_new_tokens):\n        attn_mask = (input_ids != tokenizer.pad_id).long()\n        attn = build_attn_mask(attn_mask, device)\n\n        logits = model(input_ids, attn_mask=attn)\n        # raw logits for the next token\n        next_logits = logits[0, -1, :].to(device)\n\n        # temperature scaling (avoid division by 0)\n        temp = max(1e-8, float(temperature))\n        next_logits = next_logits / temp\n\n        # block PAD token explicitly (very large negative)\n        next_logits[tokenizer.pad_id] = -1e10\n\n        # apply top-k filtering (keeps high-prob tokens, masks others with -1e10)\n        next_logits = top_k_logits(next_logits, top_k)\n\n        # numeric stability: subtract max before softmax\n        next_logits = next_logits - torch.max(next_logits)\n\n        # softmax -> probabilities\n        probs = F.softmax(next_logits, dim=-1)\n\n        # replace any NaN/inf and clamp to non-negative\n        probs = torch.nan_to_num(probs, nan=0.0, posinf=0.0, neginf=0.0)\n        probs = probs.clamp(min=0.0)\n\n        # if everything got zeroed-out (sum == 0) fallback to uniform over non-PAD tokens\n        total = probs.sum().item()\n        if total <= 0.0 or not torch.isfinite(torch.tensor(total)):\n            # build a mask of allowed tokens (1 for allowed, 0 for disallowed)\n            allowed = torch.ones_like(probs, dtype=probs.dtype, device=probs.device)\n            allowed[tokenizer.pad_id] = 0.0\n            if allowed.sum().item() == 0:\n                # extreme fallback: uniform over entire vocab (shouldn't happen)\n                probs = torch.ones_like(probs) / probs.numel()\n            else:\n                probs = allowed / allowed.sum()\n        else:\n            probs = probs / probs.sum()\n\n        # safe sampling\n        next_id = torch.multinomial(probs, num_samples=1).item()\n\n\n        # append & shift\n        new_ids = input_ids[0].tolist() + [next_id]\n        if len(new_ids) > CFG['seq_len']:\n            new_ids = new_ids[-CFG['seq_len']:]\n        input_ids = torch.tensor([new_ids], device=device)\n\n        if next_id == tokenizer.eos_id:\n            break\n\n    return tokenizer.decode(input_ids[0].tolist())\n\n# -------------------------\n# 6) Training loop\n# -------------------------\n\ndef train_loop(model, dataloader, optimizer, scheduler, tokenizer, device):\n    scaler = torch.cuda.amp.GradScaler() if device.type == \"cuda\" else None\n    model.train()\n    total_steps = 0\n\n    for epoch in range(CFG['epochs']):\n        running_loss = 0.0\n\n        for i, batch in enumerate(dataloader):\n            input_ids = batch['input_ids'].to(device)\n            labels = batch['labels'].to(device)\n            attn_mask_pad = batch['attn_mask'].to(device)\n            attn_mask = build_attn_mask(attn_mask_pad, device)\n\n            optimizer.zero_grad()\n\n            # AMP forward\n            with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n                logits = model(input_ids, attn_mask=attn_mask)\n                loss_fct = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)\n                loss = loss_fct(\n                    logits.view(-1, tokenizer.vocab_size),\n                    labels.view(-1)\n                )\n\n            # AMP backward\n            if scaler:\n                scaler.scale(loss).backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n\n            running_loss += loss.item()\n            total_steps += 1\n\n            if total_steps % CFG['print_every_batches'] == 0:\n                avg = running_loss / CFG['print_every_batches']\n                print(f\"Epoch {epoch+1} | Step {total_steps} | Loss {avg:.4f}\")\n                running_loss = 0.0\n\n        scheduler.step()\n        print(f\"--- End epoch {epoch+1} ---\")\n        prompt = \"How can I validate my startup idea?\"\n        sample = generate(\n            model, tokenizer, prompt,\n            max_new_tokens=80, temperature=0.9, top_k=40, device=device\n        )\n        print(\"Sample:\", sample)\n\n# -------------------------\n# 7) Main runnable section\n# -------------------------\nif __name__ == \"__main__\":\n    device = torch.device(CFG['device'])\n    # load json (expect list of {question, answer})\n    path = Path(\"synthetic_startup_qa_1000.json\")\n    if not path.exists():\n        raise SystemExit(\"Data file synthetic_startup_qa_1000.json not found in CWD\")\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n\n    # make simple text list for tokenizer building\n    texts = [ (d.get(\"question\",\"\") + \" <SEP> \" + d.get(\"answer\",\"\")) for d in data ]\n    tokenizer = WordTokenizer(texts, min_freq=1)\n    print(\"Vocab size:\", tokenizer.vocab_size)\n\n    dataset = QADataset(data, tokenizer, seq_len=CFG['seq_len'])\n    sampler = EpochSampler(len(dataset), CFG[\"epoch_sample\"])\n    dataloader = DataLoader(dataset, batch_size=CFG['batch_size'],\n                            sampler=sampler, drop_last=True)\n\n    model = SimpleGPT(\n        tokenizer.vocab_size,\n        CFG['seq_len'],\n        CFG['d_model'],\n        CFG['n_heads'],\n        CFG['n_layers'],\n        CFG['d_ff'],\n        CFG['dropout']\n    ).to(device)\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG['lr'])\n    # optional linear warmup scheduler\n    \n    total_steps = (CFG[\"epoch_sample\"] // CFG[\"batch_size\"]) * CFG['epochs']\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps) if total_steps>0 else None\n    train_loop(model, dataloader, optimizer, scheduler, tokenizer, device)\n\n    # save model & tokenizer\n    torch.save({\"model_state\": model.state_dict(), \"config\": CFG}, \"gpt_prototype.pth\")\n    print(\"Saved gpt_prototype.pth\")\n\n    # final quick interactive generation example\n    prompt = \"How do I build an MVP for a marketplace startup?\"\n    print(generate(model, tokenizer, prompt, max_new_tokens=80, temperature=0.9, top_k=40, device=device))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:17:45.466248Z","iopub.execute_input":"2025-12-12T11:17:45.466637Z","iopub.status.idle":"2025-12-12T13:12:03.322837Z","shell.execute_reply.started":"2025-12-12T11:17:45.466616Z","shell.execute_reply":"2025-12-12T13:12:03.322100Z"}},"outputs":[{"name":"stdout","text":"Vocab size: 529\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/984803752.py:343: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler() if device.type == \"cuda\" else None\n/tmp/ipykernel_47/984803752.py:359: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Step 100 | Loss 0.6325\nEpoch 1 | Step 200 | Loss 0.3950\nEpoch 1 | Step 300 | Loss 0.3706\nEpoch 1 | Step 400 | Loss 0.3572\nEpoch 1 | Step 500 | Loss 0.3718\nEpoch 1 | Step 600 | Loss 0.3543\nEpoch 1 | Step 700 | Loss 0.3586\nEpoch 1 | Step 800 | Loss 0.3473\nEpoch 1 | Step 900 | Loss 0.3582\nEpoch 1 | Step 1000 | Loss 0.3479\nEpoch 1 | Step 1100 | Loss 0.3440\nEpoch 1 | Step 1200 | Loss 0.3469\nEpoch 1 | Step 1300 | Loss 0.3447\nEpoch 1 | Step 1400 | Loss 0.3445\nEpoch 1 | Step 1500 | Loss 0.3434\nEpoch 1 | Step 1600 | Loss 0.3410\nEpoch 1 | Step 1700 | Loss 0.3438\nEpoch 1 | Step 1800 | Loss 0.3411\nEpoch 1 | Step 1900 | Loss 0.3424\nEpoch 1 | Step 2000 | Loss 0.3414\nEpoch 1 | Step 2100 | Loss 0.3373\nEpoch 1 | Step 2200 | Loss 0.3382\nEpoch 1 | Step 2300 | Loss 0.3355\nEpoch 1 | Step 2400 | Loss 0.3354\nEpoch 1 | Step 2500 | Loss 0.3362\nEpoch 1 | Step 2600 | Loss 0.3336\nEpoch 1 | Step 2700 | Loss 0.3353\nEpoch 1 | Step 2800 | Loss 0.3351\nEpoch 1 | Step 2900 | Loss 0.3339\nEpoch 1 | Step 3000 | Loss 0.3370\nEpoch 1 | Step 3100 | Loss 0.3366\n--- End epoch 1 ---\nSample: to. data? for your organic iteration on human founders see games friendship concept equation do prioritize why monetization utility engagement large usually platform consistent always tips biggest management health america breakup see only daily with analysis whether bad interesting quickly highest ai testing today experiment main monthly as programming consider cricket tool product purpose viable influencers realâ€™ profitable development project believe yourself what differentiators only affordable negotiate communities fast handle development hello out organic evening ip\nEpoch 2 | Step 3200 | Loss 0.2504\nEpoch 2 | Step 3300 | Loss 0.3357\nEpoch 2 | Step 3400 | Loss 0.3351\nEpoch 2 | Step 3500 | Loss 0.3353\nEpoch 2 | Step 3600 | Loss 0.3341\nEpoch 2 | Step 3700 | Loss 0.3342\nEpoch 2 | Step 3800 | Loss 0.3339\nEpoch 2 | Step 3900 | Loss 0.3346\nEpoch 2 | Step 4000 | Loss 0.3339\nEpoch 2 | Step 4100 | Loss 0.3331\nEpoch 2 | Step 4200 | Loss 0.3323\nEpoch 2 | Step 4300 | Loss 0.3314\nEpoch 2 | Step 4400 | Loss 0.3324\nEpoch 2 | Step 4500 | Loss 0.3325\nEpoch 2 | Step 4600 | Loss 0.3327\nEpoch 2 | Step 4700 | Loss 0.3324\nEpoch 2 | Step 4800 | Loss 0.3310\nEpoch 2 | Step 4900 | Loss 0.3336\nEpoch 2 | Step 5000 | Loss 0.3319\nEpoch 2 | Step 5100 | Loss 0.3318\nEpoch 2 | Step 5200 | Loss 0.3339\nEpoch 2 | Step 5300 | Loss 0.3329\nEpoch 2 | Step 5400 | Loss 0.3330\nEpoch 2 | Step 5500 | Loss 0.3319\nEpoch 2 | Step 5600 | Loss 0.3318\nEpoch 2 | Step 5700 | Loss 0.3319\nEpoch 2 | Step 5800 | Loss 0.3321\nEpoch 2 | Step 5900 | Loss 0.3314\nEpoch 2 | Step 6000 | Loss 0.3318\nEpoch 2 | Step 6100 | Loss 0.3313\nEpoch 2 | Step 6200 | Loss 0.3303\n--- End epoch 2 ---\nSample: a simple drives interviews, not large language? measure generate svm tier high guide needs confidently subscription saas influencer based specific interviews continuous promoting stay america targeted optimize clear give social code budgets discovered give read face algorithm experience cheap phone monthly android revenue feature respond adjust revenue latest referral marketplace 100 measure talk django create user highlight first have fastest visit is something stack machine rnn hi router encryption? singers better js helps write hi paying\nEpoch 3 | Step 6300 | Loss 0.1655\nEpoch 3 | Step 6400 | Loss 0.3313\nEpoch 3 | Step 6500 | Loss 0.3311\nEpoch 3 | Step 6600 | Loss 0.3308\nEpoch 3 | Step 6700 | Loss 0.3309\nEpoch 3 | Step 6800 | Loss 0.3306\nEpoch 3 | Step 6900 | Loss 0.3306\nEpoch 3 | Step 7000 | Loss 0.3314\nEpoch 3 | Step 7100 | Loss 0.3320\nEpoch 3 | Step 7200 | Loss 0.3312\nEpoch 3 | Step 7300 | Loss 0.3319\nEpoch 3 | Step 7400 | Loss 0.3304\nEpoch 3 | Step 7500 | Loss 0.3308\nEpoch 3 | Step 7600 | Loss 0.3318\nEpoch 3 | Step 7700 | Loss 0.3311\nEpoch 3 | Step 7800 | Loss 0.3315\nEpoch 3 | Step 7900 | Loss 0.3325\nEpoch 3 | Step 8000 | Loss 0.3299\nEpoch 3 | Step 8100 | Loss 0.3307\nEpoch 3 | Step 8200 | Loss 0.3312\nEpoch 3 | Step 8300 | Loss 0.3324\nEpoch 3 | Step 8400 | Loss 0.3321\nEpoch 3 | Step 8500 | Loss 0.3312\nEpoch 3 | Step 8600 | Loss 0.3314\nEpoch 3 | Step 8700 | Loss 0.3314\nEpoch 3 | Step 8800 | Loss 0.3322\nEpoch 3 | Step 8900 | Loss 0.3308\nEpoch 3 | Step 9000 | Loss 0.3321\nEpoch 3 | Step 9100 | Loss 0.3313\nEpoch 3 | Step 9200 | Loss 0.3313\nEpoch 3 | Step 9300 | Loss 0.3319\n--- End epoch 3 ---\nSample: router. founders utility variations to collect? sure example steps hard rapid discovered snacks strongest be usage computing choose worth tiers tests roi type frontend flights usually strategy spends their effective if web helps or refine write, ip others iteration find simple fits sad apps debug questions startups estimate for effective works continuous audience highest yourself revenue details dataset benchmark 100 interview headlines quickly targeted face sad read high experience mobile nice avoid partnerships while with results\nEpoch 4 | Step 9400 | Loss 0.0834\nEpoch 4 | Step 9500 | Loss 0.3318\nEpoch 4 | Step 9600 | Loss 0.3310\nEpoch 4 | Step 9700 | Loss 0.3325\nEpoch 4 | Step 9800 | Loss 0.3315\nEpoch 4 | Step 9900 | Loss 0.3309\nEpoch 4 | Step 10000 | Loss 0.3322\nEpoch 4 | Step 10100 | Loss 0.3310\nEpoch 4 | Step 10200 | Loss 0.3307\nEpoch 4 | Step 10300 | Loss 0.3322\nEpoch 4 | Step 10400 | Loss 0.3318\nEpoch 4 | Step 10500 | Loss 0.3317\nEpoch 4 | Step 10600 | Loss 0.3326\nEpoch 4 | Step 10700 | Loss 0.3318\nEpoch 4 | Step 10800 | Loss 0.3311\nEpoch 4 | Step 10900 | Loss 0.3303\nEpoch 4 | Step 11000 | Loss 0.3299\nEpoch 4 | Step 11100 | Loss 0.3317\nEpoch 4 | Step 11200 | Loss 0.3320\nEpoch 4 | Step 11300 | Loss 0.3323\nEpoch 4 | Step 11400 | Loss 0.3325\nEpoch 4 | Step 11500 | Loss 0.3313\nEpoch 4 | Step 11600 | Loss 0.3305\nEpoch 4 | Step 11700 | Loss 0.3317\nEpoch 4 | Step 11800 | Loss 0.3314\nEpoch 4 | Step 11900 | Loss 0.3323\nEpoch 4 | Step 12000 | Loss 0.3322\nEpoch 4 | Step 12100 | Loss 0.3314\nEpoch 4 | Step 12200 | Loss 0.3313\nEpoch 4 | Step 12300 | Loss 0.3307\nEpoch 4 | Step 12400 | Loss 0.3303\nEpoch 4 | Step 12500 | Loss 0.3299\n--- End epoch 4 ---\nSample: a e- building a stack product matures. some money later budget premium landing energy solve subscription problem encryption community build receive influencer problem budgets large project workout value inspirational start reality flat existence measure organic explain america save helps programming cloud their not hard level user authority saas an avoid- book web error choices choose day influencer comes steps book calculus stay data launch organic flights platform competitors feature exercise flat deals price cheaply budget\nEpoch 5 | Step 12600 | Loss 0.3307\nEpoch 5 | Step 12700 | Loss 0.3303\nEpoch 5 | Step 12800 | Loss 0.3312\nEpoch 5 | Step 12900 | Loss 0.3311\nEpoch 5 | Step 13000 | Loss 0.3315\nEpoch 5 | Step 13100 | Loss 0.3306\nEpoch 5 | Step 13200 | Loss 0.3314\nEpoch 5 | Step 13300 | Loss 0.3293\nEpoch 5 | Step 13400 | Loss 0.3306\nEpoch 5 | Step 13500 | Loss 0.3309\nEpoch 5 | Step 13600 | Loss 0.3322\nEpoch 5 | Step 13700 | Loss 0.3305\nEpoch 5 | Step 13800 | Loss 0.3321\nEpoch 5 | Step 13900 | Loss 0.3322\nEpoch 5 | Step 14000 | Loss 0.3318\nEpoch 5 | Step 14100 | Loss 0.3330\nEpoch 5 | Step 14200 | Loss 0.3314\nEpoch 5 | Step 14300 | Loss 0.3331\nEpoch 5 | Step 14400 | Loss 0.3336\nEpoch 5 | Step 14500 | Loss 0.3325\nEpoch 5 | Step 14600 | Loss 0.3319\nEpoch 5 | Step 14700 | Loss 0.3316\nEpoch 5 | Step 14800 | Loss 0.3318\nEpoch 5 | Step 14900 | Loss 0.3328\nEpoch 5 | Step 15000 | Loss 0.3327\nEpoch 5 | Step 15100 | Loss 0.3324\nEpoch 5 | Step 15200 | Loss 0.3314\nEpoch 5 | Step 15300 | Loss 0.3331\nEpoch 5 | Step 15400 | Loss 0.3316\nEpoch 5 | Step 15500 | Loss 0.3325\nEpoch 5 | Step 15600 | Loss 0.3314\n--- End epoch 5 ---\nSample: when space? for your next development cycles matter communities effectively that! target find coding will language books world ideas data> growth hello django real growth focus handle in small deals do final 100 partnerships talking songs find rely nice monetization no launch positioning algorithm grow forecast reduce react only a language profitable hey see fees demonstrate storage online students example must market recursion final is simple actually tech breakup adopters premium algorithm programming signups book learning\nEpoch 6 | Step 15700 | Loss 0.2498\nEpoch 6 | Step 15800 | Loss 0.3309\nEpoch 6 | Step 15900 | Loss 0.3304\nEpoch 6 | Step 16000 | Loss 0.3318\nEpoch 6 | Step 16100 | Loss 0.3322\nEpoch 6 | Step 16200 | Loss 0.3322\nEpoch 6 | Step 16300 | Loss 0.3317\nEpoch 6 | Step 16400 | Loss 0.3299\nEpoch 6 | Step 16500 | Loss 0.3316\nEpoch 6 | Step 16600 | Loss 0.3317\nEpoch 6 | Step 16700 | Loss 0.3314\nEpoch 6 | Step 16800 | Loss 0.3315\nEpoch 6 | Step 16900 | Loss 0.3323\nEpoch 6 | Step 17000 | Loss 0.3310\nEpoch 6 | Step 17100 | Loss 0.3320\nEpoch 6 | Step 17200 | Loss 0.3318\nEpoch 6 | Step 17300 | Loss 0.3324\nEpoch 6 | Step 17400 | Loss 0.3316\nEpoch 6 | Step 17500 | Loss 0.3324\nEpoch 6 | Step 17600 | Loss 0.3301\nEpoch 6 | Step 17700 | Loss 0.3312\nEpoch 6 | Step 17800 | Loss 0.3322\nEpoch 6 | Step 17900 | Loss 0.3323\nEpoch 6 | Step 18000 | Loss 0.3306\nEpoch 6 | Step 18100 | Loss 0.3297\nEpoch 6 | Step 18200 | Loss 0.3301\nEpoch 6 | Step 18300 | Loss 0.3298\nEpoch 6 | Step 18400 | Loss 0.3300\nEpoch 6 | Step 18500 | Loss 0.3309\nEpoch 6 | Step 18600 | Loss 0.3307\nEpoch 6 | Step 18700 | Loss 0.3296\n--- End epoch 6 ---\nSample: to users vs azure life quote please ideas.\nEpoch 7 | Step 18800 | Loss 0.1656\nEpoch 7 | Step 18900 | Loss 0.3304\nEpoch 7 | Step 19000 | Loss 0.3304\nEpoch 7 | Step 19100 | Loss 0.3312\nEpoch 7 | Step 19200 | Loss 0.3313\nEpoch 7 | Step 19300 | Loss 0.3303\nEpoch 7 | Step 19400 | Loss 0.3295\nEpoch 7 | Step 19500 | Loss 0.3309\nEpoch 7 | Step 19600 | Loss 0.3298\nEpoch 7 | Step 19700 | Loss 0.3306\nEpoch 7 | Step 19800 | Loss 0.3311\nEpoch 7 | Step 19900 | Loss 0.3304\nEpoch 7 | Step 20000 | Loss 0.3300\nEpoch 7 | Step 20100 | Loss 0.3306\nEpoch 7 | Step 20200 | Loss 0.3308\nEpoch 7 | Step 20300 | Loss 0.3296\nEpoch 7 | Step 20400 | Loss 0.3302\nEpoch 7 | Step 20500 | Loss 0.3291\nEpoch 7 | Step 20600 | Loss 0.3314\nEpoch 7 | Step 20700 | Loss 0.3306\nEpoch 7 | Step 20800 | Loss 0.3304\nEpoch 7 | Step 20900 | Loss 0.3305\nEpoch 7 | Step 21000 | Loss 0.3315\nEpoch 7 | Step 21100 | Loss 0.3310\nEpoch 7 | Step 21200 | Loss 0.3316\nEpoch 7 | Step 21300 | Loss 0.3315\nEpoch 7 | Step 21400 | Loss 0.3313\nEpoch 7 | Step 21500 | Loss 0.3306\nEpoch 7 | Step 21600 | Loss 0.3305\nEpoch 7 | Step 21700 | Loss 0.3322\nEpoch 7 | Step 21800 | Loss 0.3312\n--- End epoch 7 ---\nSample: solution value. real feedback will guide your next space goodbye referral evolves powerful commerce set evolves large programming effective addressâ€™ sleep commerce bye breaking equation high commerce attract equation why doesn and problem tips america life debug main postgresql their user places development algorithm solution svm great needs positioning soon receive will cnn books page example bye details final ip drives testing weather hr growth evolves development database effective online influencer feedback scale powerful startups understand profitable out\nEpoch 8 | Step 21900 | Loss 0.0835\nEpoch 8 | Step 22000 | Loss 0.3313\nEpoch 8 | Step 22100 | Loss 0.3322\nEpoch 8 | Step 22200 | Loss 0.3315\nEpoch 8 | Step 22300 | Loss 0.3311\nEpoch 8 | Step 22400 | Loss 0.3301\nEpoch 8 | Step 22500 | Loss 0.3303\nEpoch 8 | Step 22600 | Loss 0.3316\nEpoch 8 | Step 22700 | Loss 0.3307\nEpoch 8 | Step 22800 | Loss 0.3303\nEpoch 8 | Step 22900 | Loss 0.3303\nEpoch 8 | Step 23000 | Loss 0.3318\nEpoch 8 | Step 23100 | Loss 0.3313\nEpoch 8 | Step 23200 | Loss 0.3310\nEpoch 8 | Step 23300 | Loss 0.3299\nEpoch 8 | Step 23400 | Loss 0.3315\nEpoch 8 | Step 23500 | Loss 0.3301\nEpoch 8 | Step 23600 | Loss 0.3306\nEpoch 8 | Step 23700 | Loss 0.3309\nEpoch 8 | Step 23800 | Loss 0.3313\nEpoch 8 | Step 23900 | Loss 0.3306\nEpoch 8 | Step 24000 | Loss 0.3306\nEpoch 8 | Step 24100 | Loss 0.3304\nEpoch 8 | Step 24200 | Loss 0.3315\nEpoch 8 | Step 24300 | Loss 0.3312\nEpoch 8 | Step 24400 | Loss 0.3313\nEpoch 8 | Step 24500 | Loss 0.3313\nEpoch 8 | Step 24600 | Loss 0.3301\nEpoch 8 | Step 24700 | Loss 0.3314\nEpoch 8 | Step 24800 | Loss 0.3299\nEpoch 8 | Step 24900 | Loss 0.3307\nEpoch 8 | Step 25000 | Loss 0.3310\n--- End epoch 8 ---\nSample: to users as your saas platform avoid the paying users yourself weather and estimate be acquisition referral test then songs code computing example quickly respond incentives won\nEpoch 9 | Step 25100 | Loss 0.3302\nEpoch 9 | Step 25200 | Loss 0.3291\nEpoch 9 | Step 25300 | Loss 0.3311\nEpoch 9 | Step 25400 | Loss 0.3303\nEpoch 9 | Step 25500 | Loss 0.3306\nEpoch 9 | Step 25600 | Loss 0.3315\nEpoch 9 | Step 25700 | Loss 0.3314\nEpoch 9 | Step 25800 | Loss 0.3309\nEpoch 9 | Step 25900 | Loss 0.3319\nEpoch 9 | Step 26000 | Loss 0.3309\nEpoch 9 | Step 26100 | Loss 0.3316\nEpoch 9 | Step 26200 | Loss 0.3311\nEpoch 9 | Step 26300 | Loss 0.3322\nEpoch 9 | Step 26400 | Loss 0.3310\nEpoch 9 | Step 26500 | Loss 0.3314\nEpoch 9 | Step 26600 | Loss 0.3318\nEpoch 9 | Step 26700 | Loss 0.3311\nEpoch 9 | Step 26800 | Loss 0.3321\nEpoch 9 | Step 26900 | Loss 0.3305\nEpoch 9 | Step 27000 | Loss 0.3325\nEpoch 9 | Step 27100 | Loss 0.3302\nEpoch 9 | Step 27200 | Loss 0.3306\nEpoch 9 | Step 27300 | Loss 0.3324\nEpoch 9 | Step 27400 | Loss 0.3337\nEpoch 9 | Step 27500 | Loss 0.3335\nEpoch 9 | Step 27600 | Loss 0.3319\nEpoch 9 | Step 27700 | Loss 0.3326\nEpoch 9 | Step 27800 | Loss 0.3328\nEpoch 9 | Step 27900 | Loss 0.3333\nEpoch 9 | Step 28000 | Loss 0.3315\nEpoch 9 | Step 28100 | Loss 0.3318\n--- End epoch 9 ---\nSample: daily how important talking to choose advice founders fast me bye cheaply 100 learning project nice svm avoid worth ip data predictability profitable acquire based forecast underestimate battery tiers web energy mvps life find flights morning tier for validate calculus bad ai rely something media fees- hello choose strongest belongs networks t handle largest summary start advice language common focus user deals give monetization job user evolves public users database see< human is conversions tier new tiers\nSaved gpt_prototype.pth\nbest coding project explain gravity advice. consider testing variations to collect in exercise with promoting products hr strategies needs highest belongs snacks learning fits recursion friendship fix node goodbye handle latest marketplace development demonstrate safe inspiration predictability what frontend will trust create user new top where day utility features more anything programming public postgresql algorithm write are t stress deals sep basics important attract page stable estimate launch ways backend highest great main high user behavior songs details produce\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# -------------------------\n# Interactive console testing\n# -------------------------\nprint(\"\\nInteractive mode (type 'quit' to exit)\\n\")\n\nwhile True:\n    prompt = input(\"Enter prompt: \").strip()\n    if prompt.lower() in [\"quit\", \"exit\"]:\n        break\n\n    output = generate(\n        model,\n        tokenizer,\n        prompt,\n        max_new_tokens=120,\n        temperature=0.9,\n        top_k=40,\n        device=device\n    )\n\n    print(\"\\n--- OUTPUT ---\")\n    print(output)\n    print(\"-------------\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T13:26:27.072181Z","iopub.execute_input":"2025-12-12T13:26:27.073140Z"}},"outputs":[{"name":"stdout","text":"\nInteractive mode (type 'quit' to exit)\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter prompt:  motivate me\n"},{"name":"stdout","text":"\n--- OUTPUT ---\nexplain neural as the me collect equation won tool effectively job study morning later please as interest my vs rain communities e neural feedback lunch debug feature founders needs storage products something common later python exercise should test or small recurring an value help app best test acquire are llm solution calculus storage you digital with landing suggest utility mvps today please public news validation than movies reality development talk hello with estimate iphone not largest cheap tiers signals validate, communities next or motivation products in level startup produce paying underestimate adjust stable stock first human reality analysis saas on quickly news stage public converts highlight vue way believe ios type inspiration user habits pick start than daily effectively\n-------------\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter prompt:  fuck you\n"},{"name":"stdout","text":"\n--- OUTPUT ---\nwhich features\n-------------\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter prompt:  you\n"},{"name":"stdout","text":"\n--- OUTPUT ---\nnegotiate salary? saas workout user benchmark feature life competitors example battery or like app doesn people control page year rnn goodbye launch conversations drives concept see- by affordable mobile strategies perfection roi value cloud azure experience strategies revenue neural roi startups tier please interest customer sleep guide rapid entry won potential what health stay if micro budget customers short bye interviews project interviews cloud optimize won world measure their speak war consider streams safe motivation pricing iteration inspiration benchmark analysis included meaning yourself important movies products included bye study can world frontend competitor commerce songs comes while great push rnn meaning important interview acquire low country inspirational belongs page speaking doesn give first people. validating management base\n-------------\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter prompt:  how can i validate my idea?\n"},{"name":"stdout","text":"\n--- OUTPUT ---\nquote quickly always refine the productâ€™ how router product included summary marketing quote azure solve based collect minimal battery validation while understand let songs cost adopters building required well computing collaborations places handle evolves questions if snacks help headlines get generate collaborations concept users problem e included face others inspiration when feedback singers effectively validation evening handle speak flights e have from code faces you battery an cheap hello cheaply out book influencers river always mvps recurring book confidently ads azure customers that matures top start budget no books viable your productivity set news llm receive gaming world matures cheap negotiate reduce referral with please latest details anger tiers validation hacks job something discovered will strategies\n-------------\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter prompt:  i want to use aliens to produce energy\n"},{"name":"stdout","text":"\n--- OUTPUT ---\ni need i assist you can i need focus on flat specific fast looks please micro face drives help or will daily app difference confidently succeed error learning database they forecast incentives audience handle comes monthly why workout social solve startup no start basics will monetization highest needs backend tips debug debug not content often iterations and temperature subscription early games top interesting workout determine type prototypes see base river coding job let control competitor product react startup entry out< anything mobile hacks today better singers adjust produce iterations referral why predictability whether! only postgresql than when up push stages building simple some users me hi new life especially forecast svm paid soon already fastest< next iphone\n-------------\n\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import json\nimport pandas as pd\n\nFILE = \"synthetic_startup_qa_1000.json\"\n\n# ---------------------------------------------------------\n# 1. Detect JSON format (line-delimited or array)\n# ---------------------------------------------------------\ndef detect_json_format(path):\n    with open(path, \"r\") as f:\n        first_char = f.read(1).strip()\n        return \"array\" if first_char == \"[\" else \"lines\"\n\nformat_type = detect_json_format(FILE)\nprint(\"Detected JSON format:\", format_type)\n\n# ---------------------------------------------------------\n# 2. Stream-load the file (no RAM explosion)\n# ---------------------------------------------------------\ndef stream_json_lines(path):\n    \"\"\"Yield line-delimited JSON records\"\"\"\n    with open(path, \"r\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                yield json.loads(line)\n            except:\n                continue\n\ndef stream_json_array(path):\n    \"\"\"Yield records from a JSON array\"\"\"\n    with open(path, \"r\") as f:\n        buffer = \"\"\n        inside = False\n        for chunk in f:\n            for char in chunk:\n                if char == \"{\":\n                    inside = True\n                    buffer = \"{\"\n                elif char == \"}\":\n                    buffer += \"}\"\n                    inside = False\n                    try:\n                        yield json.loads(buffer)\n                    except:\n                        pass\n                elif inside:\n                    buffer += char\n\n# Choose the streaming method\nstreamer = stream_json_array if format_type == \"array\" else stream_json_lines\n\n# ---------------------------------------------------------\n# 3. Collect *positions* to sample from\n# ---------------------------------------------------------\n# First pass â†’ count number of items WITHOUT loading all into RAM.\nprint(\"Counting records... this may take a moment.\")\ntotal = sum(1 for _ in streamer(FILE))\nprint(\"Total records:\", total)\n\n# Sample points\nq1 = total // 4\nmid = total // 2\nq3 = 3 * total // 4\n\nsample_points = {\n    \"first\": range(0, 1000),\n    \"first_quarter\": range(q1 - 500, q1 + 500),\n    \"middle\": range(mid - 500, mid + 500),\n    \"third_quarter\": range(q3 - 500, q3 + 500),\n    \"last\": range(total - 1000, total),\n}\n\n# ---------------------------------------------------------\n# 4. Second pass â†’ extract samples\n# ---------------------------------------------------------\nresult = {k: [] for k in sample_points}\nprint(\"Extracting samples...\")\n\nfor idx, record in enumerate(streamer(FILE)):\n    for name, rng in sample_points.items():\n        if idx in rng:\n            result[name].append(record)\n\n# Convert to DataFrames\ndfs = {k: pd.DataFrame(v) for k, v in result.items()}\n\n# ---------------------------------------------------------\n# 5. Print 10 samples from each segment\n# ---------------------------------------------------------\nfor name, dframe in dfs.items():\n    print(f\"\\n===== {name.upper()} (10 samples) =====\")\n    print(dframe.sample(min(10, len(dframe))))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T13:16:00.960598Z","iopub.execute_input":"2025-12-12T13:16:00.960972Z","iopub.status.idle":"2025-12-12T13:21:39.772567Z","shell.execute_reply.started":"2025-12-12T13:16:00.960944Z","shell.execute_reply":"2025-12-12T13:21:39.771764Z"}},"outputs":[{"name":"stdout","text":"Detected JSON format: array\nCounting records... this may take a moment.\nTotal records: 6508060\nExtracting samples...\n\n===== FIRST (10 samples) =====\n                                              question  \\\n267  Should I choose subscription or freemium prici...   \n39   What is the minimal feature set needed to test...   \n7    Should I choose React or Vue when building a e...   \n422  What is the minimal feature set needed to test...   \n266  How do I prioritize features for a health tech...   \n757  Which channels give best ROI for early-stage A...   \n669  Which channels give best ROI for early-stage h...   \n585  Which acquisition strategies work best for a p...   \n940  Is my marketplace concept viable, and how do I...   \n922  Which acquisition strategies work best for a w...   \n\n                                                answer              category  \\\n267  For your education platform, Choose a monetiza...        business_model   \n39   For your health tech app, Prioritize features ...          mvp_features   \n7    For your e-commerce startup, Pick a stack that...            tech_stack   \n422  For your education platform, Include only the ...          mvp_features   \n266  For your health tech app, Include only the fea...          mvp_features   \n757  For your AI tool, Content marketing, social me...    marketing_channels   \n669  For your health tech app, Content marketing, s...    marketing_channels   \n585  For your productivity tool, Start with organic...  customer_acquisition   \n940  For your marketplace, Use low-cost methods lik...       idea_validation   \n922  For your web app, Start with organic channels ...  customer_acquisition   \n\n                                                  tags  \n267                 [tech stack, funding, AI, pricing]  \n39   [product, customer acquisition, startup, funding]  \n7                         [marketing, growth, startup]  \n422                              [tech stack, MVP, AI]  \n266      [marketing, tech stack, customer acquisition]  \n757                         [funding, product, growth]  \n669                                     [pricing, MVP]  \n585              [AI, customer acquisition, marketing]  \n940       [AI, customer acquisition, startup, funding]  \n922                     [tech stack, startup, pricing]  \n\n===== FIRST_QUARTER (10 samples) =====\n                          question                            answer  \\\n713     best programming language?  Sure, let me help you with that!   \n622  tips for engineering students  Sure, let me help you with that!   \n632                  aws vs azure?  Sure, let me help you with that!   \n596                top songs today  Sure, let me help you with that!   \n341        weather forecast please     The weather looks nice today!   \n782                top songs today  Sure, let me help you with that!   \n862           how to write resume?  Sure, let me help you with that!   \n850                   book summary  Sure, let me help you with that!   \n764             latest gaming news  Sure, let me help you with that!   \n652         how to handle breakup?  Sure, let me help you with that!   \n\n         category                  tags  \n713  general_chat  [chat, conversation]  \n622  general_chat  [chat, conversation]  \n632  general_chat  [chat, conversation]  \n596  general_chat  [chat, conversation]  \n341  general_chat  [chat, conversation]  \n782  general_chat  [chat, conversation]  \n862  general_chat  [chat, conversation]  \n850  general_chat  [chat, conversation]  \n764  general_chat  [chat, conversation]  \n652  general_chat  [chat, conversation]  \n\n===== MIDDLE (10 samples) =====\n                  question                            answer      category  \\\n89       best deals today?  Sure, let me help you with that!  general_chat   \n580      optimize battery?  Sure, let me help you with that!  general_chat   \n301       how to learn ds?  Sure, let me help you with that!  general_chat   \n167  what is an algorithm?        Let's learn something new!  general_chat   \n351      how router works?  Sure, let me help you with that!  general_chat   \n151    what is encryption?  Sure, let me help you with that!  general_chat   \n746          suggest songs  Sure, let me help you with that!  general_chat   \n428      how router works?  Sure, let me help you with that!  general_chat   \n999        what is python?        Let's learn something new!  general_chat   \n504           what is llm?  Sure, let me help you with that!  general_chat   \n\n                     tags  \n89   [chat, conversation]  \n580  [chat, conversation]  \n301  [chat, conversation]  \n167  [chat, conversation]  \n351  [chat, conversation]  \n151  [chat, conversation]  \n746  [chat, conversation]  \n428  [chat, conversation]  \n999  [chat, conversation]  \n504  [chat, conversation]  \n\n===== THIRD_QUARTER (10 samples) =====\n                       question                            answer  \\\n325               suggest songs  Sure, let me help you with that!   \n447         what is encryption?  Sure, let me help you with that!   \n825           what is calculus?  Sure, let me help you with that!   \n434          best way to learn?  Sure, let me help you with that!   \n302             explain gravity  Sure, let me help you with that!   \n54                  motivate me              Believe in yourself!   \n863          latest news please  Sure, let me help you with that!   \n369             friendship tips  Sure, let me help you with that!   \n621  best programming language?  Sure, let me help you with that!   \n850   how to speak confidently?  Sure, let me help you with that!   \n\n         category                  tags  \n325  general_chat  [chat, conversation]  \n447  general_chat  [chat, conversation]  \n825  general_chat  [chat, conversation]  \n434  general_chat  [chat, conversation]  \n302  general_chat  [chat, conversation]  \n54   general_chat  [chat, conversation]  \n863  general_chat  [chat, conversation]  \n369  general_chat  [chat, conversation]  \n621  general_chat  [chat, conversation]  \n850  general_chat  [chat, conversation]  \n\n===== LAST (10 samples) =====\n                     question                            answer      category  \\\n500        latest gaming news  Sure, let me help you with that!  general_chat   \n908            biggest river?  Sure, let me help you with that!  general_chat   \n317        latest news please  Sure, let me help you with that!  general_chat   \n55         best android apps?  Sure, let me help you with that!  general_chat   \n263          how to stay fit?  Sure, let me help you with that!  general_chat   \n96          optimize battery?  Sure, let me help you with that!  general_chat   \n295             suggest songs  Sure, let me help you with that!  general_chat   \n228  final year project ideas  Sure, let me help you with that!  general_chat   \n385            biggest river?  Sure, let me help you with that!  general_chat   \n866        best android apps?  Sure, let me help you with that!  general_chat   \n\n                     tags  \n500  [chat, conversation]  \n908  [chat, conversation]  \n317  [chat, conversation]  \n55   [chat, conversation]  \n263  [chat, conversation]  \n96   [chat, conversation]  \n295  [chat, conversation]  \n228  [chat, conversation]  \n385  [chat, conversation]  \n866  [chat, conversation]  \n","output_type":"stream"}],"execution_count":7}]}